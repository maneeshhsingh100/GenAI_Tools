{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies and Kaldi\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y cmake make gcc g++ git subversion python3-dev python3-pip\n",
    "!sudo apt-get install -y sox wget zlib1g-dev automake autoconf unzip\n",
    "\n",
    "!git clone https://github.com/kaldi-asr/kaldi.git\n",
    "!cd kaldi/tools && extras/install_mkl.sh\n",
    "!cd kaldi/tools && make -j 4\n",
    "!cd kaldi/src && ./configure --shared && make depend -j 4 && make -j 4\n",
    "\n",
    "# Install additional Python packages\n",
    "!pip install numpy pandas librosa\n",
    "\n",
    "# Upload your training_data folder to Colab or mount your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Write the normalize_text.py script\n",
    "normalize_text_script = \"\"\"\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define mappings for abbreviations and symbols\n",
    "abbreviation_mapping = {\n",
    "    \"mr\": \"mister\",\n",
    "    \"mrs\": \"missus\",\n",
    "    \"dr\": \"doctor\",\n",
    "    \"st\": \"saint\",\n",
    "    \"jr\": \"junior\",\n",
    "    \"sr\": \"senior\",\n",
    "    \"$\": \"dollar\",\n",
    "    \"₹\": \"rupee\",\n",
    "    \"€\": \"euro\",\n",
    "    \"£\": \"pound\",\n",
    "    \"&\": \"and\"\n",
    "}\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace abbreviations and symbols\n",
    "    for abbr, full in abbreviation_mapping.items():\n",
    "        text = re.sub(r'\\\\b' + re.escape(abbr) + r'\\\\b', full, text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove remaining punctuation\n",
    "    text = re.sub(r'\\\\d+', lambda x: num2words(int(x.group())), text)  # Convert numbers to words\n",
    "    \n",
    "    return text\n",
    "\n",
    "def num2words(number):\n",
    "    # Simplified number to words conversion; consider using the num2words library for full conversion\n",
    "    words = {\n",
    "        0: 'zero', 1: 'one', 2: 'two', 3: 'three', 4: 'four', 5: 'five', \n",
    "        6: 'six', 7: 'seven', 8: 'eight', 9: 'nine', 10: 'ten', 11: 'eleven', \n",
    "        12: 'twelve', 13: 'thirteen', 14: 'fourteen', 15: 'fifteen', 16: 'sixteen', \n",
    "        17: 'seventeen', 18: 'eighteen', 19: 'nineteen', 20: 'twenty', 30: 'thirty', \n",
    "        40: 'forty', 50: 'fifty', 60: 'sixty', 70: 'seventy', 80: 'eighty', 90: 'ninety'\n",
    "    }\n",
    "    \n",
    "    if number < 20:\n",
    "        return words[number]\n",
    "    elif number < 100:\n",
    "        return words[number // 10 * 10] + ('' if number % 10 == 0 else ' ' + words[number % 10])\n",
    "    else:\n",
    "        return str(number)  # For simplicity, handle numbers less than 100 only\n",
    "\n",
    "def normalize_files(text_dir):\n",
    "    for filename in os.listdir(text_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            filepath = os.path.join(text_dir, filename)\n",
    "            with open(filepath, 'r') as file:\n",
    "                text = file.read()\n",
    "            normalized_text = normalize_text(text)\n",
    "            with open(filepath, 'w') as file:\n",
    "                file.write(normalized_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text_dir = \"training_data/text_files\"\n",
    "    normalize_files(text_dir)\n",
    "\"\"\"\n",
    "\n",
    "# Write the script to a file\n",
    "with open('normalize_text.py', 'w') as f:\n",
    "    f.write(normalize_text_script)\n",
    "\n",
    "# Run the normalization script\n",
    "!python3 normalize_text.py\n",
    "\n",
    "# Create the prepare_data.sh script\n",
    "prepare_data_script = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "DATA_DIR=training_data\n",
    "SPEECH_DIR=$DATA_DIR/speech_files\n",
    "TEXT_DIR=$DATA_DIR/text_files\n",
    "DEST_DIR=data/local\n",
    "\n",
    "mkdir -p $DEST_DIR\n",
    "\n",
    "# Create wav.scp\n",
    "find $SPEECH_DIR -name \"*.wav\" | while read file; do\n",
    "  utt_id=$(basename $file .wav)\n",
    "  echo \"$utt_id $file\" >> $DEST_DIR/wav.scp\n",
    "done\n",
    "\n",
    "# Create text\n",
    "find $TEXT_DIR -name \"*.txt\" | while read file; do\n",
    "  utt_id=$(basename $file .txt)\n",
    "  text=$(cat $file)\n",
    "  echo \"$utt_id $text\" >> $DEST_DIR/text\n",
    "done\n",
    "\n",
    "# Create utt2spk and spk2utt\n",
    "awk '{print $1 \" \" $1}' $DEST_DIR/wav.scp > $DEST_DIR/utt2spk\n",
    "cp $DEST_DIR/utt2spk $DEST_DIR/spk2utt\n",
    "\"\"\"\n",
    "\n",
    "# Write the script to a file\n",
    "with open('prepare_data.sh', 'w') as f:\n",
    "    f.write(prepare_data_script)\n",
    "\n",
    "# Make the script executable\n",
    "!chmod +x prepare_data.sh\n",
    "\n",
    "# Run the script\n",
    "!./prepare_data.sh\n",
    "\n",
    "# Create the required directories\n",
    "!mkdir -p exp/make_mfcc/local mfcc\n",
    "\n",
    "# Feature extraction\n",
    "!steps/make_mfcc.sh --nj 1 --cmd \"run.pl\" data/local exp/make_mfcc/local mfcc\n",
    "!steps/compute_cmvn_stats.sh data/local exp/make_mfcc/local mfcc\n",
    "\n",
    "# Create the dictionary and language models\n",
    "!utils/prepare_lang.sh data/local/dict \"<UNK>\" data/local/lang data/lang\n",
    "\n",
    "# Train the monophone model\n",
    "!steps/train_mono.sh --nj 1 --cmd \"run.pl\" data/local data/lang exp/mono\n",
    "\n",
    "# Create the graph\n",
    "!utils/mkgraph.sh data/lang_test exp/mono exp/mono/graph\n",
    "\n",
    "# Decode\n",
    "!steps/decode.sh --nj 1 --cmd \"run.pl\" exp/mono/graph data/local exp/mono/decode\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
